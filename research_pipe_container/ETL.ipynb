{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cdbbd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Engineering Project \n",
    "## ETL\n",
    "\n",
    "**Authors**: \n",
    "- Dmitri Rozgonjuk\n",
    "- Eerik Sven Puudist\n",
    "- Lisanne Siniväli\n",
    "- Cheng-Han Chung\n",
    "\n",
    "\n",
    "The aim of this script is to clean the main raw data frame and write a new, clean data frame for further use. In this notebook, the comparisons of different read- and write-methods are demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b57777",
   "metadata": {},
   "source": [
    "First, we install and import the necessary libraries from one cell (to avoid having libraries in some individual cells below). The packages and their versions to be installed will later be added to the `requirements.txt` file.\n",
    "\n",
    "We also use this section to set global environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install psycopg2 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c35746",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "097cfd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB!! run the installs from terminal\n",
    "########### Library Installations ##############\n",
    "\n",
    "################### Imports ####################\n",
    "### Data wrangling\n",
    "import pandas as pd # working with dataframes\n",
    "import numpy as np # vector operations\n",
    "\n",
    "\n",
    "### Specific-purpose libraries\n",
    "# NB! Most configure with an API key\n",
    "#from pybliometrics.scopus import AbstractRetrieval\n",
    "from habanero import Crossref # CrossRef API\n",
    "from genderize import Genderize # Gender API\n",
    "\n",
    "### Misc\n",
    "import requests\n",
    "import warnings # suppress warnings\n",
    "import os # accessing directories\n",
    "from tqdm import tqdm # track loop runtime\n",
    "from unidecode import unidecode # international encoding fo names\n",
    "\n",
    "### Custom Scripts (ETL, SQL)\n",
    "from scripts.raw_to_tables import *\n",
    "from scripts.sql_queries import *\n",
    "\n",
    "#import psycopg2\n",
    "\n",
    "########## SETTING ENV PARAMETERS ################\n",
    "warnings.filterwarnings('ignore') # suppress warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfc2b3",
   "metadata": {},
   "source": [
    "## Pipeline start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92e4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables exist...\n",
      "Tables in the working directory!\n"
     ]
    }
   ],
   "source": [
    "start_pipe = time.time() # Initialize the time of pipeline\n",
    "\n",
    "# First check if the tables are already in the system\n",
    "## If tables exist, import from .csv\n",
    "\n",
    "if os.path.exists('./tables') and len(os.listdir('./tables')) == 7: # directory + 6 tables\n",
    "    print('Tables exist...')\n",
    "    author = pd.read_csv('tables/author.csv')\n",
    "    authorshiphip = pd.read_csv('tables/authorship.csv')\n",
    "    article = pd.read_csv('tables/article.csv')\n",
    "    article_category = pd.read_csv('tables/article_category.csv')\n",
    "    category = pd.read_csv('tables/category.csv')\n",
    "    journal = pd.read_csv('tables/journal.csv')\n",
    "    print('Tables in the working directory!')\n",
    "    \n",
    "\n",
    "## If tables do not exist, pull from kaggle (or local machine), proprocess to tables\n",
    "else: \n",
    "    \n",
    "    start_etl = time.time() # Initialize the time of ETL\n",
    "    print(f'Time of pipeline start: {time.ctime(start_pipe)}')\n",
    "    print()\n",
    "    # Data ingestion\n",
    "    df = ingest_and_process(force = False)\n",
    "\n",
    "    # Prepare Pandas dataframes\n",
    "    authorship, author = authorship_author_extract(df)\n",
    "    article_category, category = article_category_category_extract(df)\n",
    "    article = article_extract(df)\n",
    "    journal = journal_extract()\n",
    "    \n",
    "    # Clean the data last time: remove all authors with NaNs or too short names\n",
    "    ## NaNs\n",
    "    author = author[~author['author_id'].isnull()]\n",
    "    nan_authors = authorship[authorship['author_id'].isnull()]['article_id'].values\n",
    "    article = article.loc[~article['article_id'].isin(nan_authors)]\n",
    "    authorship = authorship.loc[~authorship['article_id'].isin(nan_authors)]\n",
    "\n",
    "    ## Too short (< 4) names\n",
    "    author = author[~(author['author_id'].str.len() < 4)].reset_index(drop = True)\n",
    "    short_authors = authorship[(authorship['author_id'].str.len() < 4)]['article_id'].values\n",
    "    article = article.loc[~article['article_id'].isin(short_authors)].reset_index(drop = True)\n",
    "    authorship = authorship.loc[~authorship['article_id'].isin(short_authors)].reset_index(drop = True)\n",
    "    \n",
    "    ## Write .csv-s to 'tables' directory\n",
    "    ### Create the 'tables' directory\n",
    "    !mkdir tables\n",
    "    \n",
    "    ### Write the tables as csv\n",
    "    authorship.to_csv('tables/authorship.csv', index = False)\n",
    "    article_category.to_csv('tables/article_category.csv', index = False)\n",
    "    category.to_csv('tables/category.csv', index = False)\n",
    "    journal.to_csv('tables/journal.csv', index = False)\n",
    "    article.to_csv('tables/article.csv', index = False)\n",
    "    author.to_csv('tables/author.csv', index = False)\n",
    "\n",
    "    end_etl = time.time() # Endtime of ETL\n",
    "    print(f'ETL Runtime: {round(end_etl - start_etl, 6)} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8111f3a2",
   "metadata": {},
   "source": [
    "# 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables:\n",
    "## authorship\n",
    "## journal <-- augment all data (use ISSN from DOI)\n",
    "## article <-- augment with number of citations\n",
    "## author <-- augment with gender and affiliation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ee30c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Article\n",
    "In this section, we use the `requests` library to fetch the citation based onthe Crossref URL of the work's DOI. We have found that this method is faster than querying the Crossref API. We extract the work type and the number of citations that the work has received; additionally, the journal ISSN for the publication is retrieved if it is available.\n",
    "\n",
    "We want to note that although we initially also wanted to fetch author affiliation, it is not really feasible, as most of this information is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9b6836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>n_authors</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>type</th>\n",
       "      <th>n_cites</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0046</td>\n",
       "      <td>A limit relation for entropy and channel capac...</td>\n",
       "      <td>10.1063/1.2779138</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0062</td>\n",
       "      <td>On-line Viterbi Algorithm and Its Relationship...</td>\n",
       "      <td>10.1007/978-3-540-74126-8_23</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0098</td>\n",
       "      <td>Sparsely-spread CDMA - a statistical mechanics...</td>\n",
       "      <td>10.1088/1751-8113/40/41/004</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0217</td>\n",
       "      <td>Capacity of a Multiple-Antenna Fading Channel ...</td>\n",
       "      <td>10.1109/TIT.2008.2011437</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0301</td>\n",
       "      <td>Differential Recursion and Differentially Alge...</td>\n",
       "      <td>10.1145/1507244.1507252</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title  \\\n",
       "0   704.0046  A limit relation for entropy and channel capac...   \n",
       "1   704.0062  On-line Viterbi Algorithm and Its Relationship...   \n",
       "2   704.0098  Sparsely-spread CDMA - a statistical mechanics...   \n",
       "3   704.0217  Capacity of a Multiple-Antenna Fading Channel ...   \n",
       "4   704.0301  Differential Recursion and Differentially Alge...   \n",
       "\n",
       "                            doi  n_authors  journal_issn  type  n_cites  year  \n",
       "0             10.1063/1.2779138          3           NaN   NaN      NaN  2009  \n",
       "1  10.1007/978-3-540-74126-8_23          3           NaN   NaN      NaN  2010  \n",
       "2   10.1088/1751-8113/40/41/004          2           NaN   NaN      NaN  2009  \n",
       "3      10.1109/TIT.2008.2011437          2           NaN   NaN      NaN  2010  \n",
       "4       10.1145/1507244.1507252          1           NaN   NaN      NaN  2009  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article = pd.read_csv('tables/article.csv') # import if necessary\n",
    "article.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "308777c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate for citation updating!!\n",
    "\n",
    "# Use the base url\n",
    "base_url = 'http://api.crossref.org/works/'\n",
    "\n",
    "def fetch_article_augments(start_range, end_range):\n",
    "    for i in range(start_range, end_range, 1): # don't use tqdm if range specified like that...\n",
    "        try:\n",
    "            # Check if the value in work type is of len 3 ('NaN')\n",
    "            if len(article['type'].astype(str)[i]) == 3:\n",
    "                doi = base_url + article.loc[i, 'doi'] # append the doi for the base URL\n",
    "                rqst = requests.get(doi) # request by URL\n",
    "                qr_result = rqst.json() # get the json\n",
    "\n",
    "                if qr_result['status'] == 'ok': # if request successful, make the queries, update fields\n",
    "                    msg = qr_result['message']\n",
    "                    work_type = msg['type']\n",
    "\n",
    "                    article.loc[i, 'type'] = work_type # add work type\n",
    "                    article.loc[i, 'n_cites'] =  qr_result['message']['is-referenced-by-count'] # add reference count\n",
    "\n",
    "                    try: \n",
    "                        article.loc[i, 'journal_issn'] =  qr_result['message']['ISSN'][0] # add journal ISSN\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "        except:\n",
    "            continue\n",
    "    # Overwrite the csv\n",
    "    article.to_csv('tables/article.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7615c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 23 15:39:37 2022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1643948582.848778"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(time.ctime())\n",
    "start_article = time.time()\n",
    "start_range = 15001\n",
    "end_range = 20000\n",
    "\n",
    "fetch_article_augments(start_range, end_range)\n",
    "\n",
    "end_article = time.time()\n",
    "end_article - start_article/60\n",
    "\n",
    "# 5k records in appx 30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10ad78b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id                                              1607.0614\n",
       "title           A Haar Wavelet-Based Perceptual Similarity Ind...\n",
       "doi                                   10.1016/j.image.2017.11.001\n",
       "n_authors                                                       4\n",
       "journal_issn                                            0923-5965\n",
       "type                                              journal-article\n",
       "n_cites                                                     111.0\n",
       "year                                                         2017\n",
       "Name: 19999, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.iloc[19999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9651c2",
   "metadata": {},
   "source": [
    "### Journal\n",
    "In order to get the journal information, we need the journal ISSN list from the `article` table. Although journal Impact Factor are more common metrics, they are trademarked and, hence, retrieving them is not open-source. The alternative is to use SNIP - source-normalized impact per publication. This is the average number of citations per publication, corrected for differences in citation practice between research domains. Fortunately, the list of journals and their SNIP is available from the CWTS website (https://www.journalindicators.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47d75785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>n_authors</th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>type</th>\n",
       "      <th>n_cites</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.0046</td>\n",
       "      <td>A limit relation for entropy and channel capac...</td>\n",
       "      <td>10.1063/1.2779138</td>\n",
       "      <td>3</td>\n",
       "      <td>0022-2488</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704.0062</td>\n",
       "      <td>On-line Viterbi Algorithm and Its Relationship...</td>\n",
       "      <td>10.1007/978-3-540-74126-8_23</td>\n",
       "      <td>3</td>\n",
       "      <td>0302-9743</td>\n",
       "      <td>book-chapter</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704.0098</td>\n",
       "      <td>Sparsely-spread CDMA - a statistical mechanics...</td>\n",
       "      <td>10.1088/1751-8113/40/41/004</td>\n",
       "      <td>2</td>\n",
       "      <td>1751-8113</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704.0217</td>\n",
       "      <td>Capacity of a Multiple-Antenna Fading Channel ...</td>\n",
       "      <td>10.1109/TIT.2008.2011437</td>\n",
       "      <td>2</td>\n",
       "      <td>0018-9448</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>113.0</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704.0301</td>\n",
       "      <td>Differential Recursion and Differentially Alge...</td>\n",
       "      <td>10.1145/1507244.1507252</td>\n",
       "      <td>1</td>\n",
       "      <td>1529-3785</td>\n",
       "      <td>journal-article</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68878</th>\n",
       "      <td>quant-ph/9903035</td>\n",
       "      <td>Quantum Bounded Query Complexity</td>\n",
       "      <td>10.1109/CCC.1999.766273</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68879</th>\n",
       "      <td>quant-ph/9904108</td>\n",
       "      <td>Self-Testing of Universal and Fault-Tolerant S...</td>\n",
       "      <td>10.1145/335305.335402</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68880</th>\n",
       "      <td>quant-ph/9905043</td>\n",
       "      <td>Operation of universal gates in a DXD supercon...</td>\n",
       "      <td>10.1103/PhysRevA.61.042308</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68881</th>\n",
       "      <td>quant-ph/9910087</td>\n",
       "      <td>Unconditionally Secure Commitment of a Certifi...</td>\n",
       "      <td>10.1103/PhysRevA.61.042301</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68882</th>\n",
       "      <td>quant-ph/9911043</td>\n",
       "      <td>Cheat Sensitive Quantum Bit Commitment</td>\n",
       "      <td>10.1103/PhysRevLett.92.157901</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68883 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             article_id                                              title  \\\n",
       "0              704.0046  A limit relation for entropy and channel capac...   \n",
       "1              704.0062  On-line Viterbi Algorithm and Its Relationship...   \n",
       "2              704.0098  Sparsely-spread CDMA - a statistical mechanics...   \n",
       "3              704.0217  Capacity of a Multiple-Antenna Fading Channel ...   \n",
       "4              704.0301  Differential Recursion and Differentially Alge...   \n",
       "...                 ...                                                ...   \n",
       "68878  quant-ph/9903035                   Quantum Bounded Query Complexity   \n",
       "68879  quant-ph/9904108  Self-Testing of Universal and Fault-Tolerant S...   \n",
       "68880  quant-ph/9905043  Operation of universal gates in a DXD supercon...   \n",
       "68881  quant-ph/9910087  Unconditionally Secure Commitment of a Certifi...   \n",
       "68882  quant-ph/9911043             Cheat Sensitive Quantum Bit Commitment   \n",
       "\n",
       "                                 doi  n_authors journal_issn             type  \\\n",
       "0                  10.1063/1.2779138          3    0022-2488  journal-article   \n",
       "1       10.1007/978-3-540-74126-8_23          3    0302-9743     book-chapter   \n",
       "2        10.1088/1751-8113/40/41/004          2    1751-8113  journal-article   \n",
       "3           10.1109/TIT.2008.2011437          2    0018-9448  journal-article   \n",
       "4            10.1145/1507244.1507252          1    1529-3785  journal-article   \n",
       "...                              ...        ...          ...              ...   \n",
       "68878        10.1109/CCC.1999.766273          2          NaN              NaN   \n",
       "68879          10.1145/335305.335402          4          NaN              NaN   \n",
       "68880     10.1103/PhysRevA.61.042308          2          NaN              NaN   \n",
       "68881     10.1103/PhysRevA.61.042301          1          NaN              NaN   \n",
       "68882  10.1103/PhysRevLett.92.157901          2          NaN              NaN   \n",
       "\n",
       "       n_cites  year  \n",
       "0          6.0  2009  \n",
       "1          5.0  2010  \n",
       "2         26.0  2009  \n",
       "3        113.0  2010  \n",
       "4          4.0  2009  \n",
       "...        ...   ...  \n",
       "68878      NaN  2007  \n",
       "68879      NaN  2007  \n",
       "68880      NaN  2009  \n",
       "68881      NaN  2009  \n",
       "68882      NaN  2009  \n",
       "\n",
       "[68883 rows x 8 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f77aae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "cwts_data = pd.read_excel('augmentation/CWTS Journal Indicators April 2022.xlsx',\n",
    "                         sheet_name = 'Sources')\n",
    "# Fix colnames (replace spaces and lower)\n",
    "cwts_data.columns = [col.replace(' ','_').lower() for col in cwts_data.columns] \n",
    "\n",
    "# Include only 2021\n",
    "cwts_2021 = cwts_data[cwts_data['year'] == 2021].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76b3d33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "60fa3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as a table to augmentations dir\n",
    "\n",
    "# fetch from augmentations dir\n",
    "\n",
    "# list nique ISSNs to journal table\n",
    "\n",
    "# extract the journal names and SNIPs from swts data\n",
    "\n",
    "# save the journal table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f113bd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>if_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0022-2488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0302-9743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1751-8113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018-9448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1529-3785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2167-6461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1383-7621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2092-6731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>2471-2566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>2198-350X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1737 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     journal_issn journal_title if_latest\n",
       "0       0022-2488           NaN       NaN\n",
       "1       0302-9743           NaN       NaN\n",
       "2       1751-8113           NaN       NaN\n",
       "3       0018-9448           NaN       NaN\n",
       "4       1529-3785           NaN       NaN\n",
       "...           ...           ...       ...\n",
       "1732    2167-6461           NaN       NaN\n",
       "1733    1383-7621           NaN       NaN\n",
       "1734    2092-6731           NaN       NaN\n",
       "1735    2471-2566           NaN       NaN\n",
       "1736    2198-350X           NaN       NaN\n",
       "\n",
       "[1737 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal = pd.read_csv('tables/journal.csv')\n",
    "journal['journal_issn'] = article['journal_issn'].unique()\n",
    "journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e3bc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(journal)):\n",
    "    \n",
    "    issn = journal.loc[i, 'journal_issn']\n",
    "    idx = cwts_2021[cwts_2021['print_issn'] == issn].index\n",
    "    \n",
    "    if len(idx) == 0:\n",
    "                pass\n",
    "    else:\n",
    "        journal.loc[i, 'journal_title'] = cwts_2021.loc[idx, 'source_title'].values[0]\n",
    "        journal.loc[i, 'if_latest'] = cwts_2021.loc[idx, 'snip'].values[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe797d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>journal_issn</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>if_latest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0022-2488</td>\n",
       "      <td>Journal of Mathematical Physics</td>\n",
       "      <td>0.98351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0302-9743</td>\n",
       "      <td>Lecture Notes in Computer Science</td>\n",
       "      <td>0.533981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1751-8113</td>\n",
       "      <td>Journal of Physics A: Mathematical and Theoret...</td>\n",
       "      <td>0.992163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018-9448</td>\n",
       "      <td>IEEE Transactions on Information Theory</td>\n",
       "      <td>1.73502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1529-3785</td>\n",
       "      <td>ACM Transactions on Computational Logic</td>\n",
       "      <td>0.98735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2167-6461</td>\n",
       "      <td>Big Data</td>\n",
       "      <td>1.647198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>1383-7621</td>\n",
       "      <td>Journal of Systems Architecture</td>\n",
       "      <td>1.845266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>2092-6731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>2471-2566</td>\n",
       "      <td>ACM Transactions on Privacy and Security</td>\n",
       "      <td>1.985854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>2198-350X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1737 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     journal_issn                                      journal_title if_latest\n",
       "0       0022-2488                    Journal of Mathematical Physics   0.98351\n",
       "1       0302-9743                  Lecture Notes in Computer Science  0.533981\n",
       "2       1751-8113  Journal of Physics A: Mathematical and Theoret...  0.992163\n",
       "3       0018-9448            IEEE Transactions on Information Theory   1.73502\n",
       "4       1529-3785            ACM Transactions on Computational Logic   0.98735\n",
       "...           ...                                                ...       ...\n",
       "1732    2167-6461                                           Big Data  1.647198\n",
       "1733    1383-7621                    Journal of Systems Architecture  1.845266\n",
       "1734    2092-6731                                                NaN       NaN\n",
       "1735    2471-2566           ACM Transactions on Privacy and Security  1.985854\n",
       "1736    2198-350X                                                NaN       NaN\n",
       "\n",
       "[1737 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3f411",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Author: gender\n",
    "In order to query 'gender' of a given author, we first extract all valid (length > 3) first names. We acknowledge that there may be first names that are smaller than four characters in length, but given that query amount is limited, we are going with a more robust way to extract as many names as possible.\n",
    "\n",
    "To that end, we are querying the names via the Genderize.io API. It allows for querying 1500 names per day. We exract the names and probabilities, and update our own data table with these data. We then finally join the table by firstname to include the gender column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13efd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the author table\n",
    "author = pd.read_csv('tables/author.csv') \n",
    "author.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c1a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique valid first names and create a temporary df with firstname and gender\n",
    "names_genders = pd.DataFrame(np.sort(author[author['first_name'].str.len() > 4]['first_name'].unique()))\n",
    "names_genders.columns = ['first_name']\n",
    "names_genders['alph_value'] = names_genders['first_name'].str.extract('([A-Z]+)') # add a column with first letter\n",
    "names_genders = names_genders.loc[(names_genders['alph_value'].str.len() < 2)].reset_index(drop = True) # remove rows where there's more than one letter\n",
    "names_genders = pd.concat([names_genders,pd.DataFrame(columns=['gender', 'prob'])])\n",
    "names_genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ff500",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_genders_file = pd.read_csv('names_genders.csv')\n",
    "names_genders_file['first_name'] = names_genders_file['first_name'].apply(unidecode)\n",
    "names_genders_file = names_genders_file.drop_duplicates(['first_name', 'gender'])\n",
    "names_genders_file\n",
    "names_genders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_gender_from_data(external_dataset, name_var, gender_var, prob_var = None):\n",
    "    \n",
    "    # Search for names from the UCI name data set\n",
    "    for i in tqdm(range(len(names_genders))):\n",
    "\n",
    "        if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "                pass\n",
    "        else:\n",
    "            # Extract the name and letter\n",
    "            firstname = names_genders.loc[i, 'first_name']\n",
    "\n",
    "            # Search in a subset of the externalm dataset\n",
    "            idx = external_dataset[external_dataset[name_var] == firstname].index\n",
    "\n",
    "            # If no index found, no name -> do nothing (augment later with API)\n",
    "            if len(idx) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                # If there is no gender data, do nothing\n",
    "                if len(external_dataset.loc[idx, gender_var]) == 0:\n",
    "                    \n",
    "                    if prob_var == None:\n",
    "                        pass\n",
    "                    # If prob-var is provided, update the prob var\n",
    "                    else:\n",
    "                        names_genders.loc[i, 'prob'] = external_dataset.loc[idx, prob_var] # get the gender\n",
    "                else:\n",
    "                    idx = idx.values[0]\n",
    "                    names_genders.loc[i, 'gender'] = external_dataset.loc[idx, gender_var] # get the gender\n",
    "                    names_genders.loc[i, 'prob'] = 1 # set prob to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gender_from_data(names_genders_file, 'first_name', 'gender', 'prob')\n",
    "names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2928983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI dataset\n",
    "uci = pd.read_csv('uci_name_gender_dataset.csv')[['Name', 'Gender']].sort_values('Name').reset_index(drop = True)\n",
    "uci = uci[~(uci['Name'].str.len() < 3)].reset_index(drop = True) # remove too short names\n",
    "uci['alph_value'] = uci['Name'].str.extract('([A-Z]+)') # create a column for partialling\n",
    "uci = uci.loc[(uci['alph_value'].str.len() < 2)] # remove rows where there's more than one letter\n",
    "uci['Name'] = uci['Name'].apply(unidecode)\n",
    "uci['Name'] = uci['Name'].str.replace('[^a-zA-Z0-9]', '', regex=True).str.strip()\n",
    "uci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a257cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gender_from_data(uci, 'Name', 'Gender')\n",
    "names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_names_table(start_n, names_genders):\n",
    "    \n",
    "    # For loop querying the genderize.io API\n",
    "    for i in tqdm(range(start_n, len(names_genders), 1)):\n",
    "        # Extract the name\n",
    "        first_name = names_genders.loc[i, 'first_name'] # first name\n",
    "        # Check if the name has already been checked\n",
    "        ## Query only if the name hasn't been checked already\n",
    "        if names_genders.loc[i, 'prob'] >= 0 and names_genders.loc[i, 'prob'] <= 1:\n",
    "            pass\n",
    "        else:\n",
    "            try: \n",
    "                gender_info = Genderize().get([first_name])\n",
    "                names_genders.loc[i, 'gender'] = gender_info[0]['gender']\n",
    "                names_genders.loc[i,'prob'] = gender_info[0]['probability']\n",
    "            except:\n",
    "                print(f'Iteration nr {i}')\n",
    "                print('Limit likely exceeded.')\n",
    "                break\n",
    "            finally:\n",
    "                # Write to csv once no more pulls are possible\n",
    "                names_genders.to_csv('names_genders.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_names_table(3106, names_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbabaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of yet not checked names: {names_genders['gender'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f31eb7",
   "metadata": {},
   "source": [
    "#### Merge author-names-genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gender table\n",
    "names_genders = pd.read_csv('names_genders.csv')\n",
    "# Exclude the names that were not found\n",
    "found_names = names_genders[names_genders['prob']>0]\n",
    "# Gender values to 'M' and 'F'\n",
    "found_names['gender'] = found_names['gender'].replace(to_replace=['male','female'], value=['M', 'F'])\n",
    "\n",
    "author = author.merge(found_names[['first_name', 'gender']], on = ['first_name'], how = 'right')\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d316af",
   "metadata": {},
   "source": [
    "# 3. From Pandas to PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Pandas\n",
    "authorship = pd.read_csv('tables/authorship.csv')\n",
    "article_category = pd.read_csv('tables/article_category.csv')\n",
    "category = pd.read_csv('tables/category.csv')\n",
    "article = pd.read_csv('tables/article.csv')\n",
    "author = pd.read_csv('tables/author.csv')\n",
    "journal = pd.read_csv('tables/journal.csv')\n",
    "\n",
    "tables = [authorship, article_category, category, article, author, journal]\n",
    "\n",
    "# Name of tables (for later print)\n",
    "authorship.name = 'authorship'\n",
    "article_category.name = 'article_category'\n",
    "category.name = 'category'\n",
    "article.name = 'article'\n",
    "author.name = 'author'\n",
    "journal.name = 'journal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ccbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa85217",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9af238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = psycopg2.connect(host=\"postgres\", user=\"postgres\", password=\"password\", database=\"postgres\")\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# create sparkify database with UTF8 encoding\n",
    "cur.execute(\"DROP DATABASE IF EXISTS research_db\")\n",
    "cur.execute(\"CREATE DATABASE research_db WITH ENCODING 'utf8' TEMPLATE template0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289253f",
   "metadata": {},
   "source": [
    "## Load the possiblity to run magic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:password@postgres/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb193368",
   "metadata": {},
   "source": [
    "# Drop Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5685088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Tables \n",
    "for query in drop_tables:\n",
    "    cur.execute(query)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f6ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that a table, e.g., 'jounal', is not in the database\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11baa737",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in create_tables:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09176459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the tables (e.g., 'author') are created\n",
    "## Should be empty\n",
    "%sql SELECT * FROM journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f7717",
   "metadata": {},
   "source": [
    "# Insert into Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_tables(table, query):\n",
    "    ''' Helper function for inserting values to Postresql tables\n",
    "    Args:\n",
    "        table (pd.DataFrame): pandas table\n",
    "        query (SQL query): correspondive SQL query for 'table' for data insertion in DB\n",
    "    '''\n",
    "    \n",
    "    print(f'Inserting table -- {table.name} -- ...')\n",
    "    \n",
    "    try:\n",
    "        for i, row in table.iterrows():\n",
    "            cur.execute(query, list(row))\n",
    "        print(f'Table -- {table.name} -- successfully inserted!')\n",
    "    except:\n",
    "        print(f'Error with table -- {table.name} --')\n",
    "    print()\n",
    "        \n",
    "for  i in range(len(tables)):\n",
    "    insert_to_tables(tables[i], insert_tables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM author LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056caaf3",
   "metadata": {},
   "source": [
    "# Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM authorship LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article_category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM article LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e72788",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM category LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM journal LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d241ee",
   "metadata": {},
   "source": [
    "# 4. Preparing Graph DB Data\n",
    "In essence, we need to (a) rename the attributes to be compliant with Neo4J notation, and (b) save the above-created tables to .csv-s: https://medium.com/@st3llasia/analyzing-arxiv-data-using-neo4j-part-1-ccce072a2027\n",
    "\n",
    "- about network analysis with these data in Neo4J: https://medium.com/swlh/network-analysis-of-arxiv-dataset-to-create-a-search-and-recommendation-engine-of-articles-cd18b36a185e\n",
    "\n",
    "- link prediction: https://towardsdatascience.com/link-prediction-with-neo4j-part-2-predicting-co-authors-using-scikit-learn-78b42356b44c\n",
    "\n",
    "The Graph Database Schema is pictured below:\n",
    "<img src=\"images/graph_db_schema.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f72785",
   "metadata": {},
   "source": [
    "# 5. Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca45e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c2278a6",
   "metadata": {},
   "source": [
    "## 5.1. Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7eb4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3deb59c",
   "metadata": {},
   "source": [
    "## 5.2. Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f2997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ef30e5f",
   "metadata": {},
   "source": [
    "## Total Pipeline Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4475ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_pipe = time.time()\n",
    "\n",
    "print(f'Time of pipeline start: {time.ctime(end_pipe)}')\n",
    "print(f'Total pipeline runtime: {(end_pipe - start_pipe)/60} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
